{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Retrival"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from pymongo import MongoClient\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from langchain.schema import Document\n",
    "from langchain.chains import ConversationalRetrievalChain\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.docstore.document import Document\n",
    "from langchain.llms import HuggingFacePipeline\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline, BitsAndBytesConfig\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "mongo_token = os.getenv(\"MONGODB_URL\")\n",
    "MODEL_ID = \"sentence-transformers/all-MiniLM-L6-v2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SentenceTransformer(MODEL_ID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# embeddings = HuggingFaceEmbeddings(model_name=MODEL_ID, model_kwargs={\"device\": \"cpu\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    client = MongoClient(mongo_token)\n",
    "    db = client['Tour']\n",
    "    collection = db['spots']\n",
    "except Exception as e:\n",
    "    print(f\"Error connecting to MongoDB: {e}\")\n",
    "    exit(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/unk/.local/lib/python3.10/site-packages/langchain_core/_api/deprecation.py:139: LangChainDeprecationWarning: The class `HuggingFacePipeline` was deprecated in LangChain 0.0.37 and will be removed in 0.3. An updated version of the class exists in the langchain-huggingface package and should be used instead. To use it run `pip install -U langchain-huggingface` and import as `from langchain_huggingface import HuggingFacePipeline`.\n",
      "  warn_deprecated(\n"
     ]
    }
   ],
   "source": [
    "def initialize_local_llm():\n",
    "    try:\n",
    "        model_name = \"distilgpt2\"  # A smaller model suitable for CPU\n",
    "        \n",
    "        tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "        \n",
    "        pipe = pipeline(\n",
    "            \"text-generation\",\n",
    "            model=model, \n",
    "            tokenizer=tokenizer, \n",
    "            max_length=512,\n",
    "            temperature=0.7,\n",
    "            top_p=0.95,\n",
    "            repetition_penalty=1.15,\n",
    "            device=-1  # This tells it to use CPU\n",
    "        )\n",
    "        \n",
    "        local_llm = HuggingFacePipeline(pipeline=pipe)\n",
    "        return local_llm\n",
    "    except Exception as e:\n",
    "        print(f\"Error initializing LLM: {e}\")\n",
    "        exit(1)\n",
    "\n",
    "llm = initialize_local_llm()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_encoder(query):\n",
    "    return model.encode([query])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_chatbot(vectorstore):\n",
    "    memory = ConversationBufferMemory(memory_key=\"chat_history\", return_messages=True)\n",
    "    qa_chain = ConversationalRetrievalChain.from_llm(\n",
    "        llm=llm,\n",
    "        retriever=vectorstore.as_retriever(),\n",
    "        memory=memory\n",
    "    )\n",
    "    return qa_chain\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_vector_store(documents):\n",
    "    docs = [\n",
    "        Document(\n",
    "            page_content=f\"{doc['name']}: {doc['description']}\",\n",
    "            metadata={\"name\": doc['name']}\n",
    "        ) for doc in documents\n",
    "    ]\n",
    "    \n",
    "    embeddings = model.encode([doc.page_content for doc in docs])\n",
    "    vector_store = FAISS.from_embeddings(embeddings, [doc.page_content for doc in docs])\n",
    "    return vector_store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_documents_from_mongodb():\n",
    "    try:\n",
    "        return list(collection.find())\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading documents from MongoDB: {e}\")\n",
    "        return []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "too many values to unpack (expected 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m documents \u001b[38;5;241m=\u001b[39m load_documents_from_mongodb()\n\u001b[0;32m----> 2\u001b[0m vector_store \u001b[38;5;241m=\u001b[39m \u001b[43mcreate_vector_store\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdocuments\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m chatbot \u001b[38;5;241m=\u001b[39m setup_chatbot(vector_store)\n",
      "Cell \u001b[0;32mIn[15], line 10\u001b[0m, in \u001b[0;36mcreate_vector_store\u001b[0;34m(documents)\u001b[0m\n\u001b[1;32m      2\u001b[0m docs \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m      3\u001b[0m     Document(\n\u001b[1;32m      4\u001b[0m         page_content\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdoc[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mname\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdoc[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdescription\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m      5\u001b[0m         metadata\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mname\u001b[39m\u001b[38;5;124m\"\u001b[39m: doc[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mname\u001b[39m\u001b[38;5;124m'\u001b[39m]}\n\u001b[1;32m      6\u001b[0m     ) \u001b[38;5;28;01mfor\u001b[39;00m doc \u001b[38;5;129;01min\u001b[39;00m documents\n\u001b[1;32m      7\u001b[0m ]\n\u001b[1;32m      9\u001b[0m embeddings \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mencode([doc\u001b[38;5;241m.\u001b[39mpage_content \u001b[38;5;28;01mfor\u001b[39;00m doc \u001b[38;5;129;01min\u001b[39;00m docs])\n\u001b[0;32m---> 10\u001b[0m vector_store \u001b[38;5;241m=\u001b[39m \u001b[43mFAISS\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_embeddings\u001b[49m\u001b[43m(\u001b[49m\u001b[43membeddings\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43mdoc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpage_content\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdoc\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdocs\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m vector_store\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/langchain_community/vectorstores/faiss.py:1006\u001b[0m, in \u001b[0;36mFAISS.from_embeddings\u001b[0;34m(cls, text_embeddings, embedding, metadatas, ids, **kwargs)\u001b[0m\n\u001b[1;32m    977\u001b[0m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[1;32m    978\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfrom_embeddings\u001b[39m(\n\u001b[1;32m    979\u001b[0m     \u001b[38;5;28mcls\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    984\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[1;32m    985\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m FAISS:\n\u001b[1;32m    986\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Construct FAISS wrapper from raw documents.\u001b[39;00m\n\u001b[1;32m    987\u001b[0m \n\u001b[1;32m    988\u001b[0m \u001b[38;5;124;03m    This is a user friendly interface that:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1004\u001b[0m \u001b[38;5;124;03m            faiss = FAISS.from_embeddings(text_embedding_pairs, embeddings)\u001b[39;00m\n\u001b[1;32m   1005\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1006\u001b[0m     texts, embeddings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39mtext_embeddings)\n\u001b[1;32m   1007\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m__from(\n\u001b[1;32m   1008\u001b[0m         \u001b[38;5;28mlist\u001b[39m(texts),\n\u001b[1;32m   1009\u001b[0m         \u001b[38;5;28mlist\u001b[39m(embeddings),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1013\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m   1014\u001b[0m     )\n",
      "\u001b[0;31mValueError\u001b[0m: too many values to unpack (expected 2)"
     ]
    }
   ],
   "source": [
    "documents = load_documents_from_mongodb()\n",
    "vector_store = create_vector_store(documents)\n",
    "chatbot = setup_chatbot(vector_store)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'vector_search' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m query \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTell me about the best spots in Karachi\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      2\u001b[0m query_embedding \u001b[38;5;241m=\u001b[39m query_encoder(query)\n\u001b[0;32m----> 3\u001b[0m search_results \u001b[38;5;241m=\u001b[39m \u001b[43mvector_search\u001b[49m(query_embedding)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'vector_search' is not defined"
     ]
    }
   ],
   "source": [
    "query = \"Tell me about the best spots in Karachi\"\n",
    "query_embedding = query_encoder(query)\n",
    "\n",
    "print(\"\\nChatbot Response:\")\n",
    "try:\n",
    "    response = chatbot({\"question\": query})\n",
    "    print(f\"Question: {query}\")\n",
    "    print(f\"Answer: {response['answer']}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error processing query: {e}\")\n",
    "\n",
    "# Cleanup\n",
    "client.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
